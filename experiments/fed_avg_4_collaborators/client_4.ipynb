{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45b64891",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydicom, re\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import flwr as fl\n",
    "from os import listdir\n",
    "from os.path import isfile, join, exists\n",
    "\n",
    "# from ipynb.fs.full.utils import MRIDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d53dc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIENT_NUM = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36c27770",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MRIDataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, df, X_col, y_col, batch_size,\n",
    "                 input_size= (256, 256), depth_size=64,\n",
    "                 shuffle=True):\n",
    " \n",
    "        self.df = df.copy()\n",
    "        self.X_col = X_col\n",
    "        self.y_col = y_col\n",
    "        self.depth_size = depth_size\n",
    "        self.batch_size = batch_size\n",
    "        self.input_size = input_size\n",
    "        self.shuffle = shuffle\n",
    "        self.n = len(self.df)\n",
    " \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            self.df = self.df.sample(frac=1).reset_index(drop=True)\n",
    " \n",
    "    def __get_input(self, path, target_size):\n",
    "        scan3d = None\n",
    "        onlyfiles = [f for f in listdir(path) if isfile(join(path, f))]\n",
    "        filepatt = 'Image-{}.dcm'\n",
    "        digits = [int(re.search('\\d+',i).group()) for i in listdir(path) if re.match(filepatt.format('\\d+\\\\'),i)]\n",
    "        digits.sort()\n",
    "        onlyfiles = [filepatt.format(dig) for dig in digits]\n",
    "        \n",
    "        center = len(onlyfiles) // 2\n",
    "        left = max(0, center - (self.depth_size // 2))\n",
    "        right = min(len(onlyfiles), center + (self.depth_size // 2))\n",
    "        onlyfiles = onlyfiles[left: right]\n",
    "        if len(onlyfiles) < self.depth_size:\n",
    "            img_shape = pydicom.read_file(f'{path}{onlyfiles[0]}').pixel_array.shape\n",
    "            add_z = self.depth_size - len(onlyfiles)\n",
    "            scan3d = np.zeros((add_z, target_size[0], target_size[1],1))\n",
    "        \n",
    "            \n",
    "        scans = []\n",
    "        for f in onlyfiles:\n",
    "            img = pydicom.read_file(f'{path}{f}')\n",
    "            img = img.pixel_array\n",
    "            img = self._rescale(img)\n",
    "            img = np.expand_dims(img, axis=-1)\n",
    "            img = tf.image.resize(img,(target_size[0], target_size[1])).numpy()\n",
    "            \n",
    "            img = self._normalize(img)\n",
    "            scans.append(img)\n",
    "        \n",
    "        if scan3d is not None:\n",
    "            return np.concatenate([np.array(scans), scan3d]) \n",
    "        else:\n",
    "            return np.array(scans)\n",
    "    def _rescale(self, arr):\n",
    "        arr_min = arr.min()\n",
    "        arr_max = arr.max()\n",
    "        if (arr_max - arr_min) == 0:\n",
    "            return arr\n",
    "        return (arr - arr_min) / (arr_max - arr_min)\n",
    "    def _normalize(self, arr):\n",
    "        img = arr - arr.mean()\n",
    "        # divide by the standard deviation (only if it is different from zero)\n",
    "        if np.std(img) != 0:\n",
    "            img = img / np.std(img)\n",
    "        return img\n",
    "    def __get_data(self, batches):\n",
    "        if self.X_col is None:\n",
    "            PATHS = ['FLAIR_path', 'T1w_path', 'T2w_path', 'T1wCE_path']\n",
    "            X_batch = []\n",
    "            for p in PATHS:\n",
    "                batch_part_path = batches[p]\n",
    "                X_batch.append(np.asarray([self.__get_input(x,  self.input_size) for x in batch_part_path]))\n",
    "            y_batch = batches[self.y_col].values\n",
    "            X_batch = np.concatenate(X_batch, axis=4)\n",
    "            \n",
    "        else:\n",
    "            path_batch = batches[self.X_col]\n",
    "            X_batch = np.asarray([self.__get_input(x,  self.input_size) for x in path_batch])\n",
    "            y_batch = batches[self.y_col].values\n",
    "        return X_batch, y_batch\n",
    "    def __getitem__(self, index):\n",
    "        batches = self.df[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        X, y = self.__get_data(batches)\n",
    "        return X, y\n",
    "    def __len__(self):\n",
    "        return self.n // self.batch_size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9dd5577",
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkDirectoryForExistence(dirName):\n",
    "    return exists(f'./client_{CLIENT_NUM}/train/{str(dirName).zfill(5)}')\n",
    "\n",
    "def getAllExistingDirs():\n",
    "    return f'./client_{CLIENT_NUM}/train/' + train_df[train_df['BraTS21ID'].apply(checkDirectoryForExistence)]['BraTS21ID'].astype(str).str.zfill(5)\n",
    "\n",
    "train_df = pd.read_csv(f\"./train_labels.csv\")\n",
    "train_df['FLAIR_path'] = getAllExistingDirs() + '/FLAIR/'\n",
    "train_df['T1w_path'] = getAllExistingDirs() + '/T1w/' \n",
    "train_df['T2w_path'] = getAllExistingDirs() + '/T2w/' \n",
    "train_df['T1wCE_path'] = getAllExistingDirs() + '/t1wCE/'\n",
    "train_df = train_df.dropna(how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69cd0040",
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkDirectoryForExistence(dirName):\n",
    "    return exists(f'./client_{CLIENT_NUM}/test/{str(dirName).zfill(5)}')\n",
    "\n",
    "def getAllExistingDirs():\n",
    "    return f'./client_{CLIENT_NUM}/test/' + test_df[test_df['BraTS21ID'].apply(checkDirectoryForExistence)]['BraTS21ID'].astype(str).str.zfill(5)\n",
    "\n",
    "test_df = pd.read_csv(f\"./train_labels.csv\")\n",
    "test_df['FLAIR_path'] = getAllExistingDirs() + '/FLAIR/'\n",
    "test_df['T1w_path'] = getAllExistingDirs() + '/T1w/' \n",
    "test_df['T2w_path'] = getAllExistingDirs() + '/T2w/' \n",
    "test_df['T1wCE_path'] = getAllExistingDirs() + '/t1wCE/'\n",
    "test_df = test_df.dropna(how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ef2134d",
   "metadata": {},
   "outputs": [],
   "source": [
    "depth = 64\n",
    "resolution = (192, 192)\n",
    "batches = 4\n",
    "gen = MRIDataGenerator(train_df, 'FLAIR_path', 'MGMT_value', batches, resolution, depth, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2587d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66355335",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv3D, MaxPooling3D, Flatten, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv3D(32, kernel_size=(3, 3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=(64,192, 192, 1)))\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model.add(Conv3D(64, kernel_size=(3, 3, 3), activation='relu', kernel_initializer='he_uniform'))\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model.add(Conv3D(128, kernel_size=(3, 3, 3), activation='relu', kernel_initializer='he_uniform'))\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model.add(Conv3D(256, kernel_size=(3, 3, 3), activation='relu', kernel_initializer='he_uniform'))\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "model.add(Dense(64, activation='relu', kernel_initializer='he_uniform'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(\n",
    "        optimizer='adam', \n",
    "        loss='binary_crossentropy',\n",
    "        metrics=[tf.keras.metrics.BinaryAccuracy()]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c0d4f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_history = []\n",
    "val_loss = []\n",
    "val_acc = []\n",
    "\n",
    "class MRI_Classifier_Client(fl.client.NumPyClient):\n",
    "    def get_parameters(self):\n",
    "        return model.get_weights()\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "        model.set_weights(parameters)\n",
    "        history = model.fit(gen, steps_per_epoch = batches, verbose=1, epochs = 3)\n",
    "        train_history.append(history)\n",
    "        return model.get_weights(), len(train_df), {}\n",
    "    \n",
    "    def get_properties(self):\n",
    "        pass\n",
    "\n",
    "    def evaluate(self, parameters, config):\n",
    "        model.set_weights(parameters)\n",
    "        loss, accuracy = model.evaluate(MRIDataGenerator(test_df, 'FLAIR_path', 'MGMT_value', batches, resolution, depth, True))\n",
    "        val_loss.append(loss)\n",
    "        val_acc.append(accuracy)\n",
    "        return loss, len(test_df), {\"accuracy\": accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7865ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO flower 2022-05-26 21:24:52,702 | connection.py:102 | Opened insecure gRPC connection (no certificates were passed)\n",
      "DEBUG flower 2022-05-26 21:24:52,953 | connection.py:39 | ChannelConnectivity.IDLE\n",
      "DEBUG flower 2022-05-26 21:24:52,953 | connection.py:39 | ChannelConnectivity.READY\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "4/4 [==============================] - 237s 59s/step - loss: 37.2000 - binary_accuracy: 0.4375\n",
      "Epoch 2/3\n",
      "4/4 [==============================] - 238s 59s/step - loss: 0.9280 - binary_accuracy: 0.5000\n",
      "Epoch 3/3\n",
      "4/4 [==============================] - 234s 58s/step - loss: 1.0890 - binary_accuracy: 0.2500\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.6869 - binary_accuracy: 0.5000\n",
      "Epoch 1/3\n",
      "4/4 [==============================] - 237s 59s/step - loss: 0.6826 - binary_accuracy: 0.5000\n",
      "Epoch 2/3\n",
      "4/4 [==============================] - 236s 59s/step - loss: 0.6117 - binary_accuracy: 0.7500\n",
      "Epoch 3/3\n",
      "4/4 [==============================] - 234s 59s/step - loss: 0.6413 - binary_accuracy: 0.6875\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6850 - binary_accuracy: 0.5000\n",
      "Epoch 1/3\n",
      "4/4 [==============================] - 235s 59s/step - loss: 0.6742 - binary_accuracy: 0.6875\n",
      "Epoch 2/3\n",
      "4/4 [==============================] - 236s 59s/step - loss: 0.5870 - binary_accuracy: 0.6875\n",
      "Epoch 3/3\n",
      "4/4 [==============================] - 237s 59s/step - loss: 0.5917 - binary_accuracy: 0.6875\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.6487 - binary_accuracy: 0.7500\n",
      "Epoch 1/3\n",
      "4/4 [==============================] - 237s 59s/step - loss: 0.6326 - binary_accuracy: 0.6875\n",
      "Epoch 2/3\n",
      "4/4 [==============================] - 234s 59s/step - loss: 0.6885 - binary_accuracy: 0.6875\n",
      "Epoch 3/3\n",
      "4/4 [==============================] - 232s 58s/step - loss: 0.5094 - binary_accuracy: 0.7500\n",
      "1/1 [==============================] - 0s 187ms/step - loss: 0.6750 - binary_accuracy: 0.7500\n",
      "Epoch 1/3\n",
      "4/4 [==============================] - 233s 58s/step - loss: 0.6873 - binary_accuracy: 0.6875\n",
      "Epoch 2/3\n",
      "4/4 [==============================] - 234s 58s/step - loss: 0.6643 - binary_accuracy: 0.7500\n",
      "Epoch 3/3\n",
      "4/4 [==============================] - 229s 57s/step - loss: 0.5976 - binary_accuracy: 0.8750\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5623 - binary_accuracy: 0.7500\n",
      "Epoch 1/3\n",
      "4/4 [==============================] - 237s 59s/step - loss: 0.6400 - binary_accuracy: 0.6250\n",
      "Epoch 2/3\n",
      "4/4 [==============================] - 235s 59s/step - loss: 0.6562 - binary_accuracy: 0.6875\n",
      "Epoch 3/3\n",
      "4/4 [==============================] - 233s 58s/step - loss: 0.5571 - binary_accuracy: 0.6875\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.6588 - binary_accuracy: 1.0000\n",
      "Epoch 1/3\n",
      "4/4 [==============================] - 238s 60s/step - loss: 0.6884 - binary_accuracy: 0.3750\n",
      "Epoch 2/3\n",
      "4/4 [==============================] - 231s 58s/step - loss: 0.6822 - binary_accuracy: 0.7500\n",
      "Epoch 3/3\n",
      "4/4 [==============================] - 234s 58s/step - loss: 0.6320 - binary_accuracy: 0.8125\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6603 - binary_accuracy: 0.7500\n",
      "Epoch 1/3\n",
      "4/4 [==============================] - 234s 58s/step - loss: 0.6764 - binary_accuracy: 0.6875\n",
      "Epoch 2/3\n",
      "4/4 [==============================] - 232s 58s/step - loss: 0.6095 - binary_accuracy: 0.8750\n",
      "Epoch 3/3\n",
      "4/4 [==============================] - 235s 59s/step - loss: 0.4931 - binary_accuracy: 0.8125\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.6428 - binary_accuracy: 0.7500\n",
      "Epoch 1/3\n",
      "4/4 [==============================] - 236s 59s/step - loss: 0.6818 - binary_accuracy: 0.5625\n",
      "Epoch 2/3\n",
      "4/4 [==============================] - 235s 59s/step - loss: 0.6122 - binary_accuracy: 0.7500\n",
      "Epoch 3/3\n",
      "4/4 [==============================] - 229s 57s/step - loss: 0.5236 - binary_accuracy: 0.7500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5978 - binary_accuracy: 1.0000\n",
      "Epoch 1/3\n",
      "4/4 [==============================] - 233s 58s/step - loss: 0.6559 - binary_accuracy: 0.6875\n",
      "Epoch 2/3\n",
      "4/4 [==============================] - 231s 58s/step - loss: 0.5136 - binary_accuracy: 0.8125\n",
      "Epoch 3/3\n",
      "4/4 [==============================] - 233s 58s/step - loss: 0.4084 - binary_accuracy: 0.8125\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6397 - binary_accuracy: 0.7500\n"
     ]
    }
   ],
   "source": [
    "fl.client.start_numpy_client(\"localhost:8080\", client=CifarClient())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af6b40a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "77df1398f74ffb58d8ff0594f502c8c260523194fbae49936cfdcd47e90e9c31"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
